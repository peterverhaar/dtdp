{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concordances\n",
    "\n",
    "This notebook explains how you can use the dtdpTdm module to perform a number of basic text and data mining operations both at the level of individual texts and at the level of full corpora. This module was developed specifically for the 2018-2019 course *Digital Text and Data Processing*. The first few examples shall focus on analayses of individual texts. \n",
    "\n",
    "The dtdpTdm module can firstly be used to produce concordances, or keyword in context lists. In a concordance, all the occurrences of a given search term are shown in combination with words that occur before and after this term. In the dtdpTdm module, users need to supply the name of the file and the regular expression the computer needs to search for in this file. Users can can specify the length of the fragments in the concordance using the third paramater. The number that is provided thirdly determines the number of characters that will be shown before and after the search result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import dtdpTdm as dtdp\n",
    "\n",
    "dir = 'Corpus'\n",
    "fileName = 'MobyDick.txt'\n",
    "\n",
    "fullPath = join( dir , fileName )\n",
    "\n",
    "## the next line produces a list of all lines containing the regular expression\n",
    "## formatted as a KWIC list\n",
    "c = dtdp.concordance( fullPath , r'\\bwhales?' , 25 )\n",
    "\n",
    "for line in c:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation analysis\n",
    "\n",
    "In collocation analyses, programs calculate the frequencies of the words that are used within a certain distance of a provided keyword. Such analyses give an impression of the semantic context of search terms. In the dtdpTdm module, collocations can be analysed using the collocation() method. The parameters are the same as those of the concordance() method. An important difference, however, is that the third parameter determines the number of words, and not the number of characters. \n",
    "\n",
    "The function removeStopWords can also be useful in this context. It removes the most common words from given dictionary. It makes use of the standard list of stopwords that is part of NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import dtdpTdm as dtdp\n",
    "\n",
    "dir = 'Corpus'\n",
    "fileName = 'MobyDick.txt'\n",
    "fullPath = join( dir , fileName )\n",
    "\n",
    "\n",
    "freq = dtdp.collocation( fullPath , r'whales?' , 30 )\n",
    "freq = dtdp.removeStopWords(freq)\n",
    "\n",
    "for w in freq:\n",
    "    print(w , freq[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be used to visualise a given dictionary with word frequencies into a word cloud. Can you use this code to visualise the results of the collocation analysis above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud \n",
    "\n",
    "wordcloud = WordCloud( background_color=\"white\",  width=1500,height=1000, max_words= 100,relative_scaling=1,normalize_plurals=False).generate_from_frequencies(freq)\n",
    "\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the code above to work, you may need to install the wordcloud module first using the commands in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word frequencies\n",
    "\n",
    "The function calculateWordFrequencies() can be used, expectedly, to calculate the frequences of all the types (i.e. the unique words) in a given text. The function only demands the name of a file as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from os.path import join\n",
    "import dtdpTdm as dtdp\n",
    "\n",
    "dir = 'Corpus'\n",
    "fileName = 'MobyDick.txt'\n",
    "fullPath = join( dir , fileName )\n",
    "\n",
    "freq = dtdp.calculateWordFrequencies( fullPath )\n",
    "\n",
    "for w in freq:\n",
    "    print(w + ' => ' + str(freq[w]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The related method mostFrequentWords() calculates the frequencies of tokens, in exactly the same way as the method calculateWordFrequencies(). This method limits itself, however, to the most frequent words in the text. Users of this method can specify the length of the frequency dictionary by providing a number as the second parameter to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from os.path import join\n",
    "import dtdpTdm as dtdp\n",
    "\n",
    "dir = 'Corpus'\n",
    "fileName = 'MobyDick.txt'\n",
    "fullPath = join( dir , fileName )\n",
    "\n",
    "freq = dtdp.mostFrequentWords( fullPath , 50 )\n",
    "\n",
    "\n",
    "for w in freq:\n",
    "    print(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using code that was explained in Part 4 of the Python tutorial, can you visualise the 50 most frequent words as a bar chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dispersion graphs\n",
    "\n",
    "Frequencies can also be clarified in dispersion graphs. To produce such graphs, a full text firstly needs to be divided into segments. The graph that is generated provides information frequency of a particular word within each of these segments. \n",
    "\n",
    "The method divideIntoSegments() demands two parameters. The first of these is the text to be analysed. Via the second parameter, users can specify the number of segments that need to be created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from os.path import join\n",
    "import dtdpTdm as dtdp\n",
    "\n",
    "dir = 'Corpus'\n",
    "fileName = 'MobyDick.txt'\n",
    "fullPath = join( dir , fileName )\n",
    "\n",
    "segments = dtdp.divideIntoSegments( fullPath , 30 )\n",
    "\n",
    "data = dict()\n",
    "\n",
    "count = 0\n",
    "for s in segments:\n",
    "    count += 1\n",
    "    hits = re.findall( r'\\bwhale' , s , re.IGNORECASE )\n",
    "    data[ count ] = len( hits )\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "fig = plt.figure( figsize=( 12 , 4 ) )\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.plot( data.keys() , data.values() , color = '#930d08' , linestyle = 'solid')\n",
    "\n",
    "ax.set_xlabel('Section')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "ax.set_title( 'Moby Dick')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
